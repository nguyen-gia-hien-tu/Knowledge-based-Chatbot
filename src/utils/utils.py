import os
import time

import streamlit as st
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains.history_aware_retriever import create_history_aware_retriever
from langchain.chains.retrieval import create_retrieval_chain
from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader
from langchain_community.embeddings import HuggingFaceBgeEmbeddings
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_pinecone import PineconeVectorStore
from langchain_text_splitters import RecursiveCharacterTextSplitter
from pinecone import Index, Pinecone, ServerlessSpec

from configuration import settings
from utils.firebase import initialize_firebase_app


@st.cache_resource()
def setup_llm():
    """Create a Google Generative AI model.

    Returns:
        ChatGoogleGenerativeAI: The Google Generative AI Large Language Model
    """
    llm = ChatGoogleGenerativeAI(
        model="gemini-1.5-flash",
        temperature=0,
        max_tokens=None,
        timeout=None,
        max_retries=2,
    )

    return llm


@st.cache_resource()
def setup_embedding():
    """Create a Hugging Face BGE Embedding model.

    Returns:
        HuggingFaceBgeEmbeddings: The Hugging Face BGE Embedding model
    """
    model_name = "BAAI/bge-large-en-v1.5"
    model_kwargs = {"device": "cpu"}
    encode_kwargs = {"normalize_embeddings": True}
    hf_embedding = HuggingFaceBgeEmbeddings(
        model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs
    )

    return hf_embedding


@st.cache_resource()
def setup_pinecone_index():
    """Create a Pinecone index if not exists and return the index

    Returns:
        Index: The Pinecone index
    """
    # Create a Pinecone connection
    pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))

    # Create a Pinecone index
    index_name = "knowledge-based-chatbot-index"
    existing_indexes = [index_info["name"] for index_info in pc.list_indexes()]

    if index_name not in existing_indexes:
        pc.create_index(
            name=index_name,
            dimension=1024,  # The dimension of the embedding model in above cell
            metric="cosine",
            spec=ServerlessSpec(cloud="aws", region="us-east-1"),
        )
        while not pc.describe_index(index_name).status["ready"]:
            time.sleep(1)

    index = pc.Index(index_name)

    return index


@st.cache_resource()
def setup_retriever(_index: Index, _embedding: HuggingFaceBgeEmbeddings):
    """Create a retriever from a vector store generated by the Pinecone index
    and the Hugging Face BGE Embedding model.
    The vector store loads splitted documents from a directory

    Args:
        _index (Index): The Pinecone index
        _embedding (Embeddings): The Embedding model

    Returns:
        VectorStoreRetriever: The vector store retriever that has the context of
            the loaded documents
    """
    # Create a vector store
    vector_store = PineconeVectorStore(index=_index, embedding=_embedding)

    # Load documents
    loader = DirectoryLoader(
        path=settings.DOCUMENTS_DIR, glob="**/*.pdf", loader_cls=PyPDFLoader
    )

    documents = loader.load()

    # Create text splitter
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,
        chunk_overlap=200,
    )

    splits = text_splitter.split_documents(documents)

    # Delete all indexes from the vector store to start fresh if any
    results = vector_store.similarity_search_with_score("test", k=1)
    print("*" * 100)
    print(f"Result from querying vectors to delete all vectors: {results}")
    if results:
        _index.delete(delete_all=True)

    # Add documents to the vector store
    vector_store.add_documents(splits)
    # vector_store.add_documents(documents)

    # Create a retriever
    retriever = vector_store.as_retriever()

    return retriever


@st.cache_resource()
def setup_tools():
    """Setup Firebase connection, LLM, Embedding, Pinecone Index, and Retriever

    Returns:
        (ChatGoogleGenerativeAI, VectorStoreRetriever): The Large Language Model
            and the Retriever
    """
    # Initialize Firebase app
    initialize_firebase_app()

    # Create an LLM
    llm = setup_llm()

    # Create an Embedding
    hf_embedding = setup_embedding()

    # Create a Pinecone index
    index = setup_pinecone_index()

    # Create a retriever
    retriever = setup_retriever(index, hf_embedding)

    return llm, retriever


def setup_rag_chain(llm, retriever):
    """Setup a RAG chain with a history-aware retriever and a question-answering system

    Args:
        llm (ChatGoogleGenerativeAI): The Large Language Model to answer questions
        retriever (VectorStoreRetriever): The retriever to retrieve context

    Returns:
        Runnable: The RAG chain Runnable
    """
    ### Contextualize question ###
    contextualize_q_system_prompt = (
        "Given a chat history and the latest user question "
        "which might reference context in the chat history, "
        "formulate a standalone question which can be understood "
        "without the chat history. Do NOT answer the question, "
        "just reformulate it if needed and otherwise return it as is."
    )

    contextualize_q_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", contextualize_q_system_prompt),
            MessagesPlaceholder("chat_history", n_messages=10),
            ("human", "{input}"),
        ]
    )

    history_aware_retriever = create_history_aware_retriever(
        llm, retriever, contextualize_q_prompt
    )

    ### Answer question ###
    system_prompt = (
        "You are an assistant for question-answering tasks. "
        "Use the following pieces of retrieved context to answer "
        "the question. If you don't know the answer, say that you "
        "don't know. Use three sentences maximum and keep the "
        "answer concise."
        "\n\n"
        "{context}"
    )
    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
        ]
    )
    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)

    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)

    return rag_chain
