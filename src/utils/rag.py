import logging
import os
import tempfile
import time
from typing import List

import streamlit as st
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains.history_aware_retriever import create_history_aware_retriever
from langchain.chains.retrieval import create_retrieval_chain
from langchain.indexes import SQLRecordManager, index
from langchain.retrievers import ContextualCompressionRetriever, MergerRetriever
from langchain.retrievers.document_compressors import (
    DocumentCompressorPipeline,
    EmbeddingsFilter,
)
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.document_transformers import (
    EmbeddingsRedundantFilter,
    LongContextReorder,
)
from langchain_community.embeddings import HuggingFaceBgeEmbeddings
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.vectorstores import VectorStoreRetriever
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_pinecone import PineconeVectorStore
from langchain_text_splitters import RecursiveCharacterTextSplitter
from pinecone import Index, Pinecone, ServerlessSpec
from streamlit.runtime.caching import CacheResourceAPI

from configuration import settings
from utils.firebase import get_blobs_in_folder_from_storage, initialize_firebase_app

logging.basicConfig(level=logging.ERROR)
logger = logging.getLogger(__name__)


@st.cache_resource()
def setup_llm():
    """Create a Google Generative AI model.

    Returns:
        ChatGoogleGenerativeAI: The Google Generative AI Large Language Model
    """
    llm = ChatGoogleGenerativeAI(
        model="gemini-1.5-flash",
        temperature=0,
        max_tokens=None,
        timeout=None,
        max_retries=2,
    )

    return llm


@st.cache_resource()
def setup_embedding(model_name: str = "BAAI/bge-large-en-v1.5"):
    """Create a Hugging Face Embedding model.

    Args:
        model_name (str, optional):
            The model name to use. Defaults to "BAAI/bge-large-en-v1.5".

    Returns:
        HuggingFaceBgeEmbeddings: The Hugging Face BGE Embedding model
    """
    model_kwargs = {"device": "cpu", "trust_remote_code": True}
    encode_kwargs = {"normalize_embeddings": True}
    bge_embedding = HuggingFaceBgeEmbeddings(
        model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs
    )

    return bge_embedding


def setup_pinecone_index(index_name: str):
    """Create a Pinecone index if not exists and return the index

    Returns:
        Index: The Pinecone index
    """
    # Create a Pinecone connection
    pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))

    # Create a Pinecone index
    existing_indexes = [index_info["name"] for index_info in pc.list_indexes()]

    if index_name not in existing_indexes:
        pc.create_index(
            name=index_name,
            dimension=1024,  # The dimension of the embedding model in above cell
            metric="cosine",
            spec=ServerlessSpec(cloud="aws", region="us-east-1"),
        )
        while not pc.describe_index(index_name).status["ready"]:
            time.sleep(1)

    index = pc.Index(index_name)

    return index


def setup_retriever(
    _llm: ChatGoogleGenerativeAI,
    _index: Index,
    _embedding: HuggingFaceBgeEmbeddings,
    vector_db_index_name: str,
    record_manager_db_url: str,
    namespace: str,
    folder_path: str,
) -> VectorStoreRetriever | CacheResourceAPI:
    """
    Create a retriever from a vector store generated by the Pinecone index and
    the Hugging Face BGE Embedding model. The vector store loads splitted
    documents from a directory

    Args:
        _index (Index): The Pinecone index _embedding (Embeddings): The
        Embedding model namespace (str): The Pinecone namespace to search for
        documents folder_path (str): The folder path to load documents from

    Returns:
        VectorStoreRetriever: The vector store retriever that has the context of
            the loaded documents
    """
    # Create a vector store
    vector_store = PineconeVectorStore(
        index=_index, embedding=_embedding, namespace=namespace
    )

    # Setup a record manager
    record_manager_namespace = f"pinecone/{vector_db_index_name}/{namespace}"
    record_manager = SQLRecordManager(
        namespace=record_manager_namespace, db_url=record_manager_db_url
    )
    # Create a schema before using the record manager
    record_manager.create_schema()

    # Load documents
    logger.info("*" * 100)
    logger.info("Loading documents from Firebase Storage")

    documents: List[Document] = []
    files = list(
        get_blobs_in_folder_from_storage(
            # folder_path=settings.DOCUMENTS_DIR,
            folder_path=folder_path,
            return_files=True,
            return_folders=False,
            recursive=True,
        )
    )

    logger.info("*" * 100)
    logger.info(f"All files only (no folders): {[file.name for file in files]}")
    logger.info("*" * 100)

    for file in files:
        # Get metadata from the file
        metadata = file.metadata

        # Download the file to a temporary directory and load the file
        with tempfile.TemporaryDirectory() as temp_dir:
            file_path = os.path.abspath(f"{temp_dir}/{file.name}")

            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            file.download_to_filename(file_path)

            loader = PyPDFLoader(file_path)
            docs = loader.load()

            for doc in docs:
                if "source" in doc.metadata:
                    doc.metadata["source"] = file.name
                if metadata:
                    doc.metadata.update(metadata)

            documents.extend(docs)

    logger.info(f"Number of documents loaded: {len(documents)}")
    logger.info("*" * 100)

    # Create text splitter
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,
        chunk_overlap=150,
    )
    splits = text_splitter.split_documents(documents)

    # Setup indexing function with `full` deletion mode
    logger.info("*" * 100)
    logger.info(f"Number of splits: {len(splits)}")
    logger.info("*" * 100)
    logger.info("Running index function to `full` cleanup")
    logger.info("*" * 100)

    index(splits, record_manager, vector_store, cleanup="full", source_id_key="source")

    logger.info("Finished running index function to `full` cleanup")
    logger.info("*" * 100)

    # Create a base retriever
    retriever = vector_store.as_retriever()

    ######################################################################
    # Combine compressors and document transformers together
    ######################################################################

    # # Remove redundant documents
    # redundant_filter = EmbeddingsRedundantFilter(embeddings=_embedding)
    # # Filter based on relevance to the query
    # relevant_filter = EmbeddingsFilter(embeddings=_embedding, similarity_threshold=0.76)

    # # Create a compressor pipline by first splitting our docs into smaller
    # # chunks, then removing redundant documents, and then filtering based on
    # # relevance to the query
    # pipeline_compressor = DocumentCompressorPipeline(
    #     transformers=[text_splitter, redundant_filter, relevant_filter]
    # )

    # # Create a multi-query retriever to generate different queries for better
    # # results https://python.langchain.com/docs/how_to/MultiQueryRetriever/
    # multi_query_retriever = MultiQueryRetriever.from_llm(retriever=retriever, llm=_llm)
    # # Create a compressor retriever from the pipline compressor and the
    # # multi-query retriever so that only relevant information is returned
    # # https://python.langchain.com/docs/how_to/contextual_compression/#stringing-compressors-and-document-transformers-together
    # compression_retriever = ContextualCompressionRetriever(
    #     base_compressor=pipeline_compressor, base_retriever=retriever
    # )

    return retriever


@st.cache_resource()
def setup_lotr(
    _llm: ChatGoogleGenerativeAI,
    namespace: str,
    folder_path: str,
):
    """
    Create the Lord of the Retriever (LOTR) with two Pinecone indexes and two
    Embedding models

    Args:
        _llm (ChatGoogleGenerativeAI):
            The Large Language Model to answer
        Embedding model namespace (str): The Pinecone namespace to search for
        documents folder_path (str): The folder path to load documents from

    Returns:
        VectorStoreRetriever: The vector store retriever that has the context of
            the loaded documents
    """
    # Create the Pinecone indexes
    index_1 = setup_pinecone_index(settings.VECTOR_DB_INDEX_NAME)
    index_2 = setup_pinecone_index(settings.VECTOR_DB_INDEX_NAME_2)

    # Setup the Embedding models
    bge_embedding = setup_embedding("BAAI/bge-large-en-v1.5")
    stella_embedding = setup_embedding("Alibaba-NLP/gte-large-en-v1.5")

    # Create the retrievers
    bge_retriever = setup_retriever(
        _llm,
        index_1,
        bge_embedding,
        settings.VECTOR_DB_INDEX_NAME,
        settings.RECORD_MANAGER_DB_URL,
        namespace,
        folder_path,
    )
    stella_retriever = setup_retriever(
        _llm,
        index_2,
        stella_embedding,
        settings.VECTOR_DB_INDEX_NAME_2,
        settings.RECORD_MANAGER_DB_URL_2,
        namespace,
        folder_path,
    )

    # Create LOTR
    lotr = MergerRetriever(retrievers=[bge_retriever, stella_retriever])

    # Remove redundant documents
    redundant_filter = EmbeddingsRedundantFilter(embeddings=bge_embedding)
    # Filter based on relevance to the query
    relevant_filter = EmbeddingsFilter(
        embeddings=bge_embedding,
        k=5,
    )
    # Transformer to reorder documents
    reordering = LongContextReorder()

    # Create a compressor pipline by first splitting our docs into smaller
    # chunks, then removing redundant documents, and then filtering based on
    # relevance to the query
    pipeline_compressor = DocumentCompressorPipeline(
        transformers=[redundant_filter, relevant_filter, reordering]
    )
    # Create a compressor retriever from the pipline compressor so that only
    # relevant information is returned
    # https://python.langchain.com/docs/how_to/contextual_compression/#stringing-compressors-and-document-transformers-together
    compression_retriever_reordered = ContextualCompressionRetriever(
        base_compressor=pipeline_compressor, base_retriever=lotr
    )

    return compression_retriever_reordered


@st.cache_resource()
def setup_rag_tools(namespace: str, folder_path: str):
    """Setup Firebase connection, LLM, Embedding, Pinecone Index, and Retriever

    Args:
        namespace (str): The Pinecone namespace to search for documents
        folder_path (str): The folder path to load documents from

    Returns:
        (ChatGoogleGenerativeAI, VectorStoreRetriever): The Large Language Model
            and the Retriever
    """
    # Initialize Firebase app
    initialize_firebase_app()

    # Create an LLM
    llm = setup_llm()
    # Create a Pinecone index
    index = setup_pinecone_index(settings.VECTOR_DB_INDEX_NAME)
    # Create an Embedding model
    embedding = setup_embedding()

    # Create a retriever
    retriever = setup_lotr(llm, namespace, folder_path)
    # retriever = setup_retriever(
    #     llm,
    #     index,
    #     embedding,
    #     settings.VECTOR_DB_INDEX_NAME,
    #     settings.RECORD_MANAGER_DB_URL,
    #     namespace,
    #     folder_path,
    # )

    return llm, retriever


def setup_rag_chain(llm, retriever):
    """
    Setup a RAG chain with a history-aware retriever and a question-answering
    system. Do not cache this function since the chat_history constantly changes
    with each human/AI message.

    Args:
        llm (ChatGoogleGenerativeAI): The Large Language Model to answer
        questions retriever (VectorStoreRetriever): The retriever to retrieve
        context

    Returns:
        Runnable: The RAG chain Runnable
    """
    # Contextualize question
    contextualized_question_system_prompt = (
        "Given a chat history and the latest user question "
        "which might reference context in the chat history, "
        "formulate a standalone question which can be understood "
        "without the chat history. Do NOT answer the question, "
        "just reformulate it if needed and otherwise return it as is."
    )

    contextualized_question_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", contextualized_question_system_prompt),
            MessagesPlaceholder("chat_history", n_messages=20),
            ("human", "{input}"),
        ]
    )

    history_aware_retriever = create_history_aware_retriever(
        llm, retriever, contextualized_question_prompt
    )

    ### Answer question ###
    system_prompt = (
        "You are an assistant for question-answering tasks. "
        "Use the following pieces of retrieved context to answer "
        "the question. If you don't know the answer, say that you "
        "don't know. Use three sentences maximum and keep the "
        "answer concise."
        "\n\n"
        "{context}"
    )
    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
        ]
    )
    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)

    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)

    return rag_chain


@st.cache_resource()
def delete_namespace_in_vector_database(namespace: str):
    """
    Delete the namespace in the vector database. This also cleans up

    Args:
        namespace (str): _description_
    """

    # Get the Embedding
    bge_embedding = setup_embedding()

    # Get the Pinecone index
    pinecone_index = setup_pinecone_index(settings.VECTOR_DB_INDEX_NAME)

    # Get the vector store
    vector_store = PineconeVectorStore(
        index=pinecone_index, embedding=bge_embedding, namespace=namespace
    )

    # Get the record manager
    record_manager_namespace = f"pinecone/{settings.VECTOR_DB_INDEX_NAME}/{namespace}"
    record_manager = SQLRecordManager(
        namespace=record_manager_namespace, db_url=settings.RECORD_MANAGER_DB_URL
    )

    # Delete related cache in the record manager
    index([], record_manager, vector_store, cleanup="full", source_id_key="source")

    # Delete the namespace in the Pinecone vector database
    pinecone_index.delete(namespace=namespace, delete_all=True)
