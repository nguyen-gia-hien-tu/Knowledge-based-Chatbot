import logging
import os
import tempfile
import time

import streamlit as st
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains.history_aware_retriever import create_history_aware_retriever
from langchain.chains.retrieval import create_retrieval_chain
from langchain.indexes import SQLRecordManager, index
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.embeddings import HuggingFaceBgeEmbeddings
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.vectorstores import VectorStoreRetriever
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_pinecone import PineconeVectorStore
from langchain_text_splitters import RecursiveCharacterTextSplitter
from pinecone import Index, Pinecone, ServerlessSpec
from streamlit.runtime.caching import CacheResourceAPI

from configuration import settings
from utils.firebase import get_blobs_in_folder_from_storage, initialize_firebase_app

logging.basicConfig(level=logging.ERROR)
logger = logging.getLogger(__name__)


@st.cache_resource()
def setup_llm():
    """Create a Google Generative AI model.

    Returns:
        ChatGoogleGenerativeAI: The Google Generative AI Large Language Model
    """
    llm = ChatGoogleGenerativeAI(
        model="gemini-1.5-flash",
        temperature=0,
        max_tokens=None,
        timeout=None,
        max_retries=2,
    )

    return llm


@st.cache_resource()
def setup_embedding():
    """Create a Hugging Face BGE Embedding model.

    Returns:
        HuggingFaceBgeEmbeddings: The Hugging Face BGE Embedding model
    """
    model_name = "BAAI/bge-large-en-v1.5"
    model_kwargs = {"device": "cpu"}
    encode_kwargs = {"normalize_embeddings": True}
    hf_embedding = HuggingFaceBgeEmbeddings(
        model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs
    )

    return hf_embedding


@st.cache_resource()
def setup_pinecone_index():
    """Create a Pinecone index if not exists and return the index

    Returns:
        Index: The Pinecone index
    """
    # Create a Pinecone connection
    pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))

    # Create a Pinecone index
    index_name = settings.VECTOR_DB_INDEX_NAME
    existing_indexes = [index_info["name"] for index_info in pc.list_indexes()]

    if index_name not in existing_indexes:
        pc.create_index(
            name=index_name,
            dimension=1024,  # The dimension of the embedding model in above cell
            metric="cosine",
            spec=ServerlessSpec(cloud="aws", region="us-east-1"),
        )
        while not pc.describe_index(index_name).status["ready"]:
            time.sleep(1)

    index = pc.Index(index_name)

    return index


@st.cache_resource()
def setup_retriever(
    _index: Index, _embedding: HuggingFaceBgeEmbeddings
) -> VectorStoreRetriever | CacheResourceAPI:
    """Create a retriever from a vector store generated by the Pinecone index
    and the Hugging Face BGE Embedding model.
    The vector store loads splitted documents from a directory

    Args:
        _index (Index): The Pinecone index
        _embedding (Embeddings): The Embedding model

    Returns:
        VectorStoreRetriever: The vector store retriever that has the context of
            the loaded documents
    """
    # Create a vector store
    vector_store = PineconeVectorStore(index=_index, embedding=_embedding)

    # Setup a record manager
    namespace = f"pinecone/{settings.VECTOR_DB_INDEX_NAME}"
    record_manager = SQLRecordManager(
        namespace=namespace, db_url=settings.RECORD_MANAGER_DB_URL
    )
    # Create a schema before using the record manager
    record_manager.create_schema()

    # Load documents
    logger.info("*" * 100)
    logger.info("Loading documents from Firebase Storage")

    documents = []
    files = list(
        get_blobs_in_folder_from_storage(
            folder_path=settings.DOCUMENTS_DIR,
            return_files=True,
            return_folders=False,
            recursive=True,
        )
    )

    logger.info("*" * 100)
    logger.info(f"All files only (no folders): {[file.name for file in files]}")
    logger.info("*" * 100)

    for file in files:
        # Get metadata from the file
        metadata = file.metadata

        # Download the file to a temporary directory and load the file
        with tempfile.TemporaryDirectory() as temp_dir:
            file_path = os.path.abspath(f"{temp_dir}/{file.name}")

            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            file.download_to_filename(file_path)

            loader = PyPDFLoader(file_path)
            docs = loader.load()

            for doc in docs:
                if "source" in doc.metadata:
                    doc.metadata["source"] = file.name
                if metadata:
                    doc.metadata.update(metadata)

            documents.extend(docs)

    logger.info(f"Number of documents loaded: {len(documents)}")
    logger.info("*" * 100)

    # Create text splitter
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,
        chunk_overlap=200,
    )
    splits = text_splitter.split_documents(documents)

    # Setup indexing function with `full` deletion mode
    logger.info("*" * 100)
    logger.info(f"Number of splits: {len(splits)}")
    logger.info("*" * 100)
    logger.info("Running index function to `full` cleanup")
    logger.info("*" * 100)

    index(splits, record_manager, vector_store, cleanup="full", source_id_key="source")

    logger.info("Finished running index function to `full` cleanup")
    logger.info("*" * 100)

    # Create a retriever
    retriever = vector_store.as_retriever()

    return retriever


@st.cache_resource()
def setup_tools():
    """Setup Firebase connection, LLM, Embedding, Pinecone Index, and Retriever

    Returns:
        (ChatGoogleGenerativeAI, VectorStoreRetriever): The Large Language Model
            and the Retriever
    """
    # Initialize Firebase app
    initialize_firebase_app()

    # Create an LLM
    llm = setup_llm()

    # Create an Embedding
    hf_embedding = setup_embedding()

    # Create a Pinecone index
    index = setup_pinecone_index()

    # Create a retriever
    retriever = setup_retriever(index, hf_embedding)

    return llm, retriever


def setup_rag_chain(llm, retriever):
    """Setup a RAG chain with a history-aware retriever and a question-answering system

    Args:
        llm (ChatGoogleGenerativeAI): The Large Language Model to answer questions
        retriever (VectorStoreRetriever): The retriever to retrieve context

    Returns:
        Runnable: The RAG chain Runnable
    """
    # Contextualize question
    contextualized_question_system_prompt = (
        "Given a chat history and the latest user question "
        "which might reference context in the chat history, "
        "formulate a standalone question which can be understood "
        "without the chat history. Do NOT answer the question, "
        "just reformulate it if needed and otherwise return it as is."
    )

    contextualized_question_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", contextualized_question_system_prompt),
            MessagesPlaceholder("chat_history", n_messages=10),
            ("human", "{input}"),
        ]
    )

    history_aware_retriever = create_history_aware_retriever(
        llm, retriever, contextualized_question_prompt
    )

    ### Answer question ###
    system_prompt = (
        "You are an assistant for question-answering tasks. "
        "Use the following pieces of retrieved context to answer "
        "the question. If you don't know the answer, say that you "
        "don't know. Use three sentences maximum and keep the "
        "answer concise."
        "\n\n"
        "{context}"
    )
    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
        ]
    )
    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)

    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)

    return rag_chain
